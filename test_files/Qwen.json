
ModelInfo(id='Qwen/Qwen2-VL-7B-Instruct', author='Qwen', sha='a7a06a1cc11b4514ce9edcde0e3ca1d16e5ff2fc', created_at=datetime.datetime(2024, 8, 28, 9, 3, 13, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2024, 12, 6, 8, 23, 24, tzinfo=datetime.timezone.utc), private=False, disabled=False, downloads=2577635, downloads_all_time=None, gated=False, gguf=None, inference=None, likes=918, library_name='transformers', tags=['transformers', 'safetensors', 'qwen2_vl', 'image-text-to-text', 'multimodal', 'conversational', 'en', 'arxiv:2409.12191', 'arxiv:2308.12966', 'base_model:Qwen/Qwen2-VL-7B', 'base_model:finetune:Qwen/Qwen2-VL-7B', 'license:apache-2.0', 'text-generation-inference', 'endpoints_compatible', 'region:us'], pipeline_tag='image-text-to-text', mask_token=None, card_data={'base_model': ['Qwen/Qwen2-VL-7B'], 'datasets': None, 'eval_results': None, 'language': ['en'], 'library_name': 'transformers', 'license': 'apache-2.0', 'license_name': None, 'license_link': None, 'metrics': None, 'model_name': None, 'pipeline_tag': 'image-text-to-text', 'tags': ['multimodal']}, widget_data=[{'text': 'Hi, what can you help me with?'}, {'text': "Hey, let's have a conversation!"}, {'text': 'Hello there!'}, {'text': 'Hey my name is Clara! How are you?'}], model_index=None, config={'architectures': ['Qwen2VLForConditionalGeneration'], 'model_type': 'qwen2_vl', 'processor_config': {'chat_template': "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"}, 'tokenizer_config': {'bos_token': None, 'chat_template': "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}", 'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'unk_token': None}}, transformers_info=TransformersInfo(auto_model='AutoModelForImageTextToText', custom_class=None, pipeline_tag='image-text-to-text', processor='AutoProcessor'), trending_score=None, siblings=[RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='LICENSE', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='chat_template.json', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model-00001-of-00005.safetensors', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model-00002-of-00005.safetensors', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model-00003-of-00005.safetensors', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model-00004-of-00005.safetensors', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model-00005-of-00005.safetensors', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='preprocessor_config.json', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None), RepoSibling(rfilename='vocab.json', size=None, blob_id=None, lfs=None)], spaces=['KingNish/OpenGPT-4o', 'GanymedeNil/Qwen2-VL-7B', 'maxiw/Qwen2-VL-Detection', 'Djrango/qwen2vl-flux-mini-demo', 'AdrienB134/rag_ColPali_Qwen2VL', 'TIGER-Lab/MEGA-Bench', 'Rijgersberg/Qwen2-VL-7B', 'KingNish/Qwen2-VL-7B', 'Mihaiii/rag_ColPali_Qwen2VL_7B', 'xianbao/Qwen2-7B-VL-demo', 'chiayewken/multimodal-longdoc-qwen2-vl', 'AlyxTeam/Qwen2.5-Coder-7B-Instruct', 'sergiopaniego/Qwen2-VL-7B-trl-sft-ChartQA', 'nhatipoglu/demo-vit-v2', 'Rahatara/rag_ColPali_Qwen2VL', 'ffgtv3/day2', 'LOpeetu/QwenVL2Demo', 'wangrongsheng/Qwen2-VL-7B', 'Dabococo/OpenGPT-4o', 'apjanco/qwen2-vl-fmb-demo', 'BoltzmannEntropy/vlms', 'miktt55/aa', 'lukiod/streamlit_qwen', 'lukiod/test2', 'Avles7/CheckPrice', 'lukiod/dock2', 'lukiod/streamlit_qwen2_withbyaldi', 'rk404/ocr_hi_en', 'saif0001/OCR_application', 'lukiod/streamlit_qwen2_withbyaldi2', 'anvi27/ocr', 'KalkiInfinity/OCR_view', 'yyasso/OpenGPT-4o', 'yyasso/OpenGPT-4o-ai', 'SansG2003/GOT_OCR2.0', 'gauri-sharan/test-two', 'apoorvgoyalxx/OCR', 'apoorvgoyalxx/ocr2', 'Sajan/QWEN2VL_OCR_demo', 'owenmr/Qwen', 'Ishita2416/OCR_application', 'intuitive262/Doc_Reader', 'hitesh2124/OCR_App', 'sprakhil/OCRQuest-2.0', 'hitesh2124/OCR', 'hitesh2124/Project1', 'hitesh2124/Internship', 'Saurabh1207/VLM', 'hitesh2124/qweve', 'rummanparvez03/OCR-Rumman', 'Mattral/OpenGPT-4o', 'BasqueLabs/OpenGPT-4o', 'xelpmocAI/PaySlip_Demo', 'wgqme/OpenGPT-4o', 'Haojiefang/Demo', 'zainnaveed/OpenGPT-4o', 'Zubik/Qwen-Qwen2-VL-7B-Instruct', 'Zubik/Qwen2-VL-7B-Instruct', 'Fretful/OpenGPT-4o', 'bibarbibar123123/Help', 'Zubik/Qwen-Qwen2-VL-7B-Instruct2', 'Zubik/Qwen-Qwen2-VL-7B-Instruct3', 'ai-ning/Qwen-Qwen2-VL-7B-Instruct', 'Hrishi147/Qwen-Qwen2-VL-7B-Instruct', 'vcoliveira/Qwen-Qwen2-VL-7B-Instructvictor', 'zeonai/TestZeroGPU', 'luigi12345/AutoIOL-ai', 'luigi12345/Auto-IOL', 'imrb/Qwen-Qwen2-VL-7B-Instruct', 'ZinCesS/Qwen-Qwen2-VL-7B-Instruct', 'Masterdqqq/OpenGPT-4o', 'Masterdqqq/Supremo', 'jamesie/Qwen-Qwen2-VL-7B-Instruct', 'Finnspiration/OpenGPT-4o-CPU', 'arman1310600/OpenGPT-4o_1', 'Zalla666/Qwen-Qwen2-VL-7B-Instruct', 'yasserrmd/BotanicalAnalyzer', 'zhouzifei/Qwen-Qwen2-VL-7B-Instruct', 'Elyess/Qwen-Qwen2-VL-7B-Instruct', 'John6666/qwen2vl-flux-zero', 'Unpredictable01/sparrow-qwen2-vl-7b', 'cocktailpeanut/qwen2vl-flux-mini-demo', 'maha2121/everopen', 'svjack/qwen2vl-flux-mini-demo', 'kingm01/Accasm', 'blackdragon901/Qwen-Qwen2-VL-7B-Instruct', 'MartsoBodziu1994/qwen2vl-flux-mini-demo', 'Rialbox/Qwen-local-demo-test', 'DrishtiSharma/rag-colpali-qwen2vl-vs-pixtral', 'Samarthan5799/Qwen-Qwen2-VL-7B-Instruct', 'ferferefer/AI_Pentacam_AS-MS39', 'geshengli2/Qwen-Qwen2-VL-7B-Instruct', 'sujit-ait/Qwen-Qwen2-VL-7B-Instruct', 'Whalberg01/OpenGPT-4o', 'Baha-mabrouk/Doc-ai', 'Pradip100/Qwen-Qwen2-VL-7B-Instruct', 'shcho-isle/qwen', 'Pranjalgupta/Qwen-Qwen2-VL-7B-Instruct', 'Pranjalgupta/Qwen-Qwen2-VL-7B-Instruct1'], safetensors=SafeTensorsInfo(parameters={'BF16': 8291375616}, total=8291375616), security_repo_status=None)